---
layout: post
title: Online Convex Programming
use_math: true
---

This blog post discusses Martin Zinkevich's 2003 paper Online Convex Programming and Generalized Infinitesimal Gradient Ascent.

The Online Convex Programming framework introduced in Zinkevich's [2003 paper](https://people.eecs.berkeley.edu/~brecht/cs294docs/week1/03.Zinkevich.pdf){:target="_blank"} models a wide range of "decision-under-uncertainty" optimization problems. As the name suggests, OCP is a natural online extension of convex optimization.

## The Problem
At each time $$t = 1,2,\dots,T$$ an agent must pick a value $$x_t$$ from a given time-invariant compact domain $$D$$. Once the agent makes a choice, the environment reveals a convex function $$g_t$$ and the agent incurs a cost of $$g_t (x_t)$$. The goal of the agent is to minimize the total cost incurred. The _regret_ $$R$$ associated with a set of choices $$\mathbf{x} = \{ x_t \}$$ made by the agent is measured against the value of the optimal static decision as
\begin{equation}
	R(\mathbf{x}) = \sum_t g_t (x_t) - \min_{x^\* \in D} \sum_{t} g_t (x^\*)
\end{equation}
There are some important things to discuss about this problem:

1. Even if $$D = [0,1]^d$$ and $$g_t : D \to [0,1]$$, it is apparent that no deterministic Turing machine can solve the problem with a regret bound of $$\Omega(T)$$ when the regret is defined as
\begin{equation}
	R(\mathbf{x}) = \sum_t g_t (x_t) - \sum_t \min_{x \in D} g_t (x)
\end{equation}
This can be argued by the simple adverserial choice of:
\begin{equation}
	g_t (x) = \begin{cases} \\| x \\|\_\infty \quad &amp; \\| x_t \\|\_\infty \ge 0.5 \\\ 
	1 - \\| x \\|\_\infty &amp; \\| x_t \\|\_\infty \le 0.5 \end{cases}
\end{equation}
where $$x_t$$ is the choice made by the agent at time $$t$$ and $$\| \cdot \|_\infty$$ is $$\ell_\infty$$-norm. At every time, the agent incurs a cost of at least $$0.5$$ while the minimum of $$g_t$$ is always $$0$$. On the other hand, it is also apparent that a non-deterministic Turing machine can solve the problem "super-optimally" by picking $$\min_{x \in D} g_t (x)$$ at every time $t$. In this case, the regret can be negative.

2. The optimal static decision is a natural notion to define the regret against in many classes of problems. An example is that of finding the value of the optimal solution to an online constrained minimization problem. Strong Lagrangian duality allows the constrained minimization primal problem to be converted into an equivalent unconstrained maximization dual problem which is concave. The catch is that the optimal dual solution $$\lambda^*$$ cannot usually be computed causally as the Lagrange dual function cannot be computed causally. However this framework asks a crucial question - can we design a primal-dual algorithm that "learns" $$\lambda^*$$ for certain problems where the Lagrange dual function can be thought of as being revealed incrementally as $$ \mathcal{L} = \frac{1}{T} \sum_{t} g_t$$ for some $$\{ g_t \}$$. This question is the subject of discussion of [Shipra Agarwal et al](https://arxiv.org/abs/1410.7596){:target="_blank"} which I may talk about in a future blog post.

## The Greedy Projection algorithm

When is a sublinear (in $$T$$) regret bound attainable? Zinkevich shows that if $$g_t$$'s are Lipschitz, one can attain a regret bound of $$\mathcal{O}(\sqrt{T})$$ using a very simple greedy projection algorithm:
\begin{equation\*}
	x_{t+1} \leftarrow P_D(x_t - \eta_t \nabla g_t (x_t)), \quad \text{where } \eta_t = t^{-\frac{1}{2}}
\end{equation\*}
where $$P_D$$ denotes projection onto the domain $$D$$. The proof relies on two observations:

1. The convexity of $$g_t$$ and the fact that it satisfies the Lipschitz condition can be used to bound the 1-step regret ($$ g_t (x_t) - g_t (x^*)$$) as a linear function of $$(\nabla g_t) (x_t)$$.
2. The update rule of the algorithm generates an upper-bound on this linear function comprising of two components - a difference of potentials that captures the $$\ell_2$$-convergence of $$\{ \dots,x_t,x_{t+1},\dots\}$$ to $$x^*$$ and a second-order error term due to the update rule moving against the local gradient $$(\nabla g_t )(x_t)$$ and not the global gradient $$(\nabla \sum_t g_t) (x_t)$$.

Informally, the rate of decay of $$\eta_t$$ plays a role in capturing the correct tradeoff between remembering past gradient information and rate of convergence.

## Minimizing regret against dynamic strategies

A natural question to ask is - can good strategies be designed when the regret is measured against not the optimal static strategy, but the optimal dynamic strategy having a total "path length" of at-most $$L$$. The path length of a dynamic strategy $$\{ x^*_t \}$$ is defined as:
\begin{equation}
\sum_{t=1}^{T-1} \\| x_{t+1}^* - x_t^* \\|
\end{equation}
It is apparent from the arguments made previously that it not possible to get sublinear regret guarantees when $$L$$ is $$\Omega(T)$$. However the definition of path-length makes things conducive to hope to find good first order incremental algorithms. Zinkevich presents the "Lazy" greedy projection algorithm which attains a regret guarantee of $$\mathcal{O} (\sqrt{TL})$$. The algorithm generates a solution $$\{x_t\}$$ starting from an arbitrary $$x_1 \in D$$ as follows: 
\begin{equation\*}
x_{t+1} \leftarrow P_D (y_{t+1}) \quad \text{where } y_{t+1} \leftarrow y_t - \eta_t (\nabla g_t) (x_t)
\end{equation\*}
