---
layout: post
title: Online Convex Programming
use_math: true
---

This blog post discusses Martin Zinkevich's 2003 paper Online Convex Programming and Generalized Infinitesimal Gradient Ascent.

The Online Convex Programming framework introduced in Zinkevich's [2003 paper](https://people.eecs.berkeley.edu/~brecht/cs294docs/week1/03.Zinkevich.pdf){:target="_blank"} models a wide range of "decision-under-uncertainty" optimization problems. As the name suggests, OCP is a natural online extension of convex optimization. At each time $$t = 1,2,\dots,T$$ an agent must pick a value $$x_t$$ from a given time-invariant convex domain $$D$$. Once the agent makes a choice, the environment reveals a convex function $$g_t$$ and the agent incurs a cost of $$g_t (x_t)$$. The goal of the agent is to minimize the total cost incurred. The _regret_ $$R$$ associated with a set of choices $$\mathbf{x} = \{ x_t \}$$ made by the agent is measured against the value of the optimal static decision as
\begin{equation}
	R(\mathbf{x}) = \sum_t g_t (x_t) - \min_{x^\* \in D} \sum_{t} g_t (x^\*)
\end{equation}
There are some important things to discuss about this problem:

1. Even if $$D = [0,1]^d$$ and $$g_t : D \to [0,1]$$, it is apparent that no deterministic Turing machine can solve the problem with a regret bound of $$\Omega(T)$$ when the regret is defined as
\begin{equation}
	R(\mathbf{x}) = \sum_t g_t (x_t) - \sum_t \min_{x \in D} g_t (x)
\end{equation}
This can be argued by the simple adverserial choice of:
\begin{equation}
	g_t (x) = \begin{cases} \\| x \\|\_\infty \quad &amp; \\| x_t \\|\_\infty \ge 0.5 \\\ 
	1 - \\| x \\|\_\infty &amp; \\| x_t \\|\_\infty \le 0.5 \end{cases}
\end{equation}
where $$x_t$$ is the choice made by the agent at time $$t$$ and $$\| \cdot \|_\infty$$ is $$\ell_\infty$$-norm. At every time, the agent incurs a cost of at least $$0.5$$ while the minimum of $$g_t$$ is always $$0$$. On the other hand, it is also apparent that a non-deterministic Turing machine can solve the problem "super-optimally" by picking $$\min_{x \in D} g_t (x)$$ at every time $t$. In this case, the regret can be negative.

2. The optimal static decision is a natural notion to define the regret against in many classes of problems. An example is that of finding the value of the optimal solution to an online constrained minimization problem. Strong Lagrangian duality allows the constrained minimization primal problem to be converted into an equivalent unconstrained maximization dual problem which is concave. The catch is that the optimal dual solution $$\lambda^*$$ cannot usually be computed causally as the Lagrange dual function cannot be computed causally. However this framework asks a crucial question - can we design a primal-dual algorithm that "learns" $$\lambda^*$$ for certain problems where the Lagrange dual function can be thought of as being revealed incrementally as $$ \mathcal{L} = \frac{1}{T} \sum_{t} g_t$$ for some $$\{ g_t \}$$. This question is the subject of discussion in [Shipra Agarwal et al](https://arxiv.org/abs/1410.7596){:target="_blank"} which I may talk about in a future blog post.

When is a sublinear (in $$T$$) regret bound attainable? Zinkevich shows that one can attain a regret bound of $$\mathcal{O}(\sqrt{T})$$ using a very simple greedy projection algorithm:

Greedy Projection Algorithm
\begin{equation}
	x_{t+1} \to P(x_t - \eta_t \nabla g_t (x_t))
\end{equation}

{% pseudo %}
Function swap(old, new)
  remaining <- quorumSize
  success <- False
  For Each host
    result[host] <- send(host, propose(old, new))
    If result[host] = "ok"
      remaining--

  If remaining > 1+quorumSize/2
    success <- True

  For Each result
    If success
      send(host, confirm(old, new))
    Else
      send(host, cancel(old, new))
{% endpseudo %}